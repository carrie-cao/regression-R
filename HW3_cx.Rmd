---
title: "HW3 Peer Assessment"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE,warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.

From the original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been scaled for use with an ANN (by dividing by 200).

## Data Description

The data consists of the following variables:

1. *Sex*: M, F, and I (infant) (categorical)
1. *Length*: Longest shell measurement in mm (continuous)
1. *Diameter*: Perpendicular to length in mm (continuous)
1. *Height*: Height with meat in shell in mm (continuous)
1. *Whole*: Weight of whole abalone in grams (continuous)
1. *Viscera*: Gut weight (after bleeding) in grams (continuous)
1. *Shell*: Shell weight after being dried in grams (continuous)
1. *Rings*: Number of rings of the abalone -- corresponds with the age (continuous)


## Read the data

```{r,warning=FALSE, message=FALSE}
# Import library you may need
library(car)
# Read the data set
abaloneFull = read.csv("abalone.csv",head=T)
row.cnt = nrow(abaloneFull)
# Split the data into training and testing sets
abaloneTest = abaloneFull[(row.cnt-9):row.cnt,]
abalone = abaloneFull[1:(row.cnt-10),]

```

*Please use abalone as your data set for the following questions unless otherwise stated.*

# Question 1: Exploratory Data Analysis [16 points]

*Please use your best judgement when grading this question. Credit should be given to submissions that use evidence from the graphs to support the conclusion, even if it does not exactly match this solution.*

**(a) Create a box plot comparing the response variable, *Rings*, across the three sex categories.  Based on this box plot, does there appear to be a relationship between the predictor and the response?**

```{r}
boxplot(Rings~Sex,xlab="Gender", ylab='Rings',data=abalone)
```
#it seems that Infant abalone has less **Rings** than Female and Male abalone.There's no significant difference between Female and Male. Therefore,  it does show a relationship between the predictor and the response.

**(b) Create plots of the response, *Rings*, against each quantitative predictor, namely *Length*, *Diameter*, *Height*, *Whole*, *Viscera*, and *Shell*.  Describe the general trend of each plot.  Are there any potential outliers?**

```{r,warning=FALSE, message=FALSE}
library("PerformanceAnalytics")
chart.Correlation(abalone[, -1],  histogram=TRUE, pch=19)
```
```{r,warning=FALSE, message=FALSE}
par(mfrow = c(2,3))
plot(abalone$Length, abalone$Rings, xlab = "Length", ylab = "Rings",  pch = 19)

plot(abalone$Diameter, abalone$Rings, xlab = "Diameter", ylab = "Rings", data=abalone, pch = 19)

plot(abalone$Height, abalone$Rings, xlab = "Height", ylab = "Rings",  pch = 19)

plot(abalone$Whole, abalone$Rings, xlab = "Whole", ylab = "Rings",  pch = 19)

plot(abalone$Viscera, abalone$Rings, xlab = "Viscera", ylab = "Rings",  pch = 19)

plot(abalone$Shell, abalone$Rings, xlab = "Shell", ylab = "Rings",  pch = 19)

```


#it seems that each quantitative predictor,  *Length*, *Diameter*, *Height*, *Whole*, *Viscera*, and *Shell* has positive linear relationship with the response, *Rings*. There are some potential outliers, Especially in *Height*, These outliers will change the direction of the relationship dramatically.


**(c) Display the correlations between each of the variables.  Interpret the correlations in the context of the relationships of the predictors to the response and in the context  of multicollinearity.**

```{r}
round(cor(abalone[,-1],),2)
```
#From the chart of (b) and the above correlation matrix, the predicting variables are positively correlated with each other and with the response *Ring*, with all the correlations >= 0.50.This suggests a strong multicollinearity among the predictors.

**(d) Based on this exploratory analysis, is it reasonable to assume a multiple linear regression model for the relationship between *Rings* and the predictor variables?**

#Because of the multicollinearity and outliers, the assumptions of regression have been seriously violated. Therefore, we do not recommend a multiple linear regression model for the relationship between *Rings* and the predictor variables.

# Question 2: Fitting the Multiple Linear Regression Model [16 points]

*Plot the full model for Rings without transforming the response variable or predicting variables.*

**(a) Build a multiple linear regression model, called model1, using the response and all predictors.  Display the summary table of the model.**

```{r}
model1 = lm(abalone$Rings~., data=abalone)
summary(model1)

```


  
### The summary table is shown as above.
  
  

**(b) Is the overall regression significant at an $\alpha$ level of 0.01?**

#as the F-statistic is 466.5 on 8 and 4158 DF, the p-value: < 2.2e-16, therefore, the overall regression  at an $\alpha$ level of 0.01 is significant.

**(c) What is the coefficient estimate for *Viscera*? Interpret this coefficient.**

the coefficient estimator:
the coefficient estimator for *Viscera* is -2.38965. 

Interpretation:
Assume other predictors hold, as *Viscera* increase one unit, the *Rings* decrease 2.38965 units.

**(d) What is the coefficient estimate for the *Sex* category Male? Interpret this coefficient.**

the coefficient estimate for the *Sex* category Male is -0.06053.

Interpretation:
Assume other predictors hold, Given a Male abalone, the  *Rings* decrease 0.06053 units than a Female.

# Question 3: Checking for Outliers and Multicollinearity [12 points]

**(a) Create a plot for the Cook's Distances. Using a threshold Cook's Distance of 1, identify the row numbers of any outliers.**

```{r}
par(mfrow = c(2,2))
plot(model1, cook.levels = 1)
```


```{r,warning=FALSE, message=FALSE}
library("olsrr")
ols_plot_cooksd_chart(model1)
which(cooks.distance(model1)>1)
```
#From the above two plots, at Cook's distance = 1, the row numbers of  outlier is 2052



**(b) Remove the outlier(s) from the data set and create a new model, called model2, using all predictors with *Rings* as the response.  Display the summary of this model.**

```{r}
abalone1 = abalone[-2052,]
model2 = lm(abalone1$Rings~., data=abalone1)
summary(model2)
```
#the summary of this model is shown as above.

**(c) Display the VIF of each predictor for model2. Using a threshold of 10 what conclusions can you draw?**

```{r}
vif(model2)
```

#Using a threshold of 10, the VIF for *Length*, *Diameter*, *Whole*, *Viscera*, and *Shell* are all bigger than 10. This indicated that the multicollinearity exists in this dataset.

# Question 4: Checking Model Assumptions [12 points]

*Please also use the cleaned data set and model2, which have the outlier(s) removed for the following questions.*

**(a) Create scatterplots of the standardized residuals of model2 versus each quantitative predictor. Does the linearity assumption appear to hold for all predictors?**

```{r}
residualsta=rstandard(model2)
par(mfrow = c(2,3))
plot(abalone1$Length, residualsta, xlab = "Length", ylab="Standardized Residuals",  pch = 19)
abline(h = 0)
plot(abalone1$Diameter, residualsta, xlab = "Diameter", ylab="Standardized Residuals",  pch = 19)
abline(h = 0)
plot(abalone1$Height, residualsta, xlab = "Height", ylab="Standardized Residuals",  pch = 19)
abline(h = 0)
plot(abalone1$Whole, residualsta, xlab = "Whole", ylab="Standardized Residuals",  pch = 19)
abline(h = 0)
plot(abalone1$Viscera, residualsta, xlab = "Viscera", ylab="Standardized Residuals",  pch = 19)
abline(h = 0)
plot(abalone1$Shell, residualsta, xlab = "Shell", ylab="Standardized Residuals",  pch = 19)
abline(h = 0)


```
#The points on all the plots show some pattern and trend, suggesting that there is  relationship between the residuals and all predictors. Therefore, linearity assumption appear to hold for all predictors in this case.

**(b) Create a scatter plot of the standardized residuals of model2 versus the fitted values of model2.  Does the constant variance assumption appear to hold for all predictors?  Do the errors appear uncorrelated?**

```{r}

plot(model2$fitted, residualsta, xlab="Fitted Values",ylab="Standardized Residuals")
abline(0,0, col='blue')
```
# From the plot, it seems that as fitted value increases, the range of residuals increased too. The residuals do not form a "horizontal band" around the residual = 0 line. This suggests that the variances of the error terms are *not* equal. The constant variance assumption does *not* appear to hold for all predictors.  The errors appear correlated. Also, it seems some residuals "stands out" from the basic random pattern of residuals. This suggests that there are some outliers.





**(c) Create a histogram and normal QQ plot for the standardized residuals. What conclusions can you draw from these plots?**

```{r}
par(mfrow = c(1,2))
hist(residualsta, xlab="rStandardized Residuals", main = "",nclass=10,col="orange")
qqPlot(residualsta, ylab="Standardized Residuals", main = "")

```
##The histogram is “skewed right,” meaning that most of the data is distributed on the left side with a long “tail” of data extending out to the right. In the QQ plot, the point’s trend upward meaning that there is a greater concentration of data beyond the right side of a Gaussian distribution.From the two graphs, we conclude that the data is *not* normally distributed. 


# Question 5 Model Comparison [12 points]

**(a) Build a third multiple linear regression model using the cleaned data set without the outlier(s), called model3, using only *Sex*, *Length*, *Whole*, and *Shell*.  Display the summary table of the model.**

```{r}
model3 = lm(abalone1$Rings~ Sex + Length + Whole + Shell,data=abalone1)
summary(model3)
```
#the summary of table is shown as above

**(b) Compare and discuss the R-squared and Adjusted R-squared of model3 with model2.**
```{r}
summary(model2)$r.squared
summary(model2)$adj.r.squared 
summary(model3)$r.squared
summary(model3)$adj.r.squared 
```
#For model2, R-squared is 0.4801762, Adjusted R-squared is 0.4791758.For model3,R-squared is 0.4619395, Adjusted R-squared is 0.4612928.  R-squared measures the proportion of the variation in dependent variable  explained by independent variables. Adjusted R-squared adjusts the statistic based on the number of independent variables in the model.The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. It is always lower than the R-squared.By the R-squared and Adjusted R-squared, model2 seems better.

**(c) Conduct a partial F-test comparing model3 with model2. What can you conclude using an $\alpha$ level of 0.01?**

```{r}
anova(model3,model2)
```
##The result shows a Df of 3 (indicating that the full model has 3 additional parameter), and a very small p-value (< 2.2e-16). We can conclude that at  an $\alpha$ level of 0.01, the full model (model2) lead to a significantly improved fit over the reduced model (model3). 

# Question 6: Transforming the data [4 points]

**(a) Find the optimal lambda, rounded to the nearest 0.5, for a Box-Cox transformation on model2.  What transformation, if any, should be applied according to the lambda value?**

```{r}
boxCox(model2,  plotit = TRUE)
```

#By the plot, the approximate optimal $\lambda$ is equal to 0. Therefore, a log transformation can be applied accroding to the lambda value.

# Question 7: Estimation and Prediction [8 points]

**(a) Estimate Rings for the last 10 rows of data (abaloneTest) using both model2 and model3.  Compare and discuss the mean squared prediction error of both models.**




```{r}
predict.testdata = predict(model2,abaloneTest,interval=c("prediction"))
predict.testdata
### Mean Squared Prediction Error
mean((predict.testdata-abaloneTest$Rings)^2)

```
#the etimate Rings for the last 10 rows of data (abaloneTest) and Mean Squared Prediction Error using  model2 is shown as above. 





```{r}
predict.testdata3 = predict(model3,abaloneTest,interval=c("prediction"))
predict.testdata3 

### Mean Squared Prediction Error

mean((predict.testdata3- abaloneTest$Rings  )^2)
```
#the etimate Rings for the last 10 rows of data (abaloneTest) and Mean Squared Prediction Error using model3 is shown as above. 
#The Mean Squared Prediction Error for Model3 is biggerer than model2 which suggest the model2 is better. 

**(b) Suppose you have found an adult female abalone with a length of 0.5mm, a whole weight of 0.4 grams, and a shell weight of 0.3 grams. Using model3, predict the number of rings on this abalone with a 90% prediction interval.**

```{r}
new_data = data.frame( Sex='F', Length=0.5, Whole=0.4, Shell=0.3)
predict(model3, new_data, interval="prediction", level=0.90)

  
```
#the number of rings on this abalone is 14.49984, the 90% prediction interval is [10.59104, 18.40863]
